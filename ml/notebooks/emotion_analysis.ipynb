{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PulseSearch — Emotion Analysis\n",
    "Loads the `google-research-datasets/go_emotions` dataset directly, fine-tunes DistilBERT on it, then classifies song lyrics.\n",
    "\n",
    "**Output per song:**\n",
    "```json\n",
    "{\n",
    "  \"artist\": \"The Weeknd\",\n",
    "  \"title\": \"Blinding Lights\",\n",
    "  \"lyrics\": \"...\",\n",
    "  \"emotions\": [\n",
    "    { \"label\": \"joy\", \"score\": 0.72 },\n",
    "    { \"label\": \"longing\", \"score\": 0.41 }\n",
    "  ],\n",
    "  \"embedding\": [0.12, -0.34, ...]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.5.0-py3-none-any.whl (515 kB)\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.5/515.2 KB\u001b[0m \u001b[31m357.1 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    }
   ],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load go_emotions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 'simplified' split uses 28 emotions (vs raw which has 27 + neutral collapsed differently)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m ds \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle-research-datasets/go_emotions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimplified\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 'simplified' split uses 28 emotions (vs raw which has 27 + neutral collapsed differently)\n",
    "ds = load_dataset(\"google-research-datasets/go_emotions\", \"simplified\")\n",
    "\n",
    "print(ds)\n",
    "print(\"\\nExample entry:\")\n",
    "print(ds[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 28 emotion labels in order — indices from the dataset map to these\n",
    "EMOTION_LABELS = [\n",
    "    \"admiration\", \"amusement\", \"anger\", \"annoyance\", \"approval\",\n",
    "    \"caring\", \"confusion\", \"curiosity\", \"desire\", \"disappointment\",\n",
    "    \"disapproval\", \"disgust\", \"embarrassment\", \"excitement\", \"fear\",\n",
    "    \"gratitude\", \"grief\", \"joy\", \"love\", \"nervousness\",\n",
    "    \"optimism\", \"pride\", \"realization\", \"relief\", \"remorse\",\n",
    "    \"sadness\", \"surprise\", \"neutral\"\n",
    "]\n",
    "\n",
    "NUM_LABELS = len(EMOTION_LABELS)\n",
    "print(f\"{NUM_LABELS} emotion labels:\", EMOTION_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare dataset for multi-label classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_CHECKPOINT = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "def encode(batch):\n",
    "    encoded = tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    # Convert list of label indices → multi-hot vector\n",
    "    multi_hot = [[1.0 if i in label_list else 0.0 for i in range(NUM_LABELS)]\n",
    "                 for label_list in batch[\"labels\"]]\n",
    "    encoded[\"labels\"] = multi_hot\n",
    "    return encoded\n",
    "\n",
    "tokenized = ds.map(encode, batched=True, remove_columns=[\"text\", \"id\"])\n",
    "tokenized.set_format(\"torch\")\n",
    "\n",
    "print(\"Tokenized dataset:\", tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tune DistilBERT on go_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport numpy as np\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_CHECKPOINT,\n    num_labels=NUM_LABELS,\n    problem_type=\"multi_label_classification\",\n    id2label={i: l for i, l in enumerate(EMOTION_LABELS)},\n    label2id={l: i for i, l in enumerate(EMOTION_LABELS)},\n)\n\ntraining_args = TrainingArguments(\n    output_dir=\"../go_emotions_model\",\n    num_train_epochs=3,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=64,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    logging_steps=100,\n    fp16=torch.cuda.is_available(),   # use GPU half-precision if available (Databricks)\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized[\"train\"],\n    eval_dataset=tokenized[\"validation\"],\n)\n\ntrainer.train()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save fine-tuned model — saves to ml/go_emotions_model/ for use by main.py\nmodel.save_pretrained(\"../go_emotions_model\")\ntokenizer.save_pretrained(\"../go_emotions_model\")\nprint(\"Model saved to ml/go_emotions_model\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classify lyrics with the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import pipeline\n\n# Load from saved checkpoint (re-run from here after training)\nemotion_classifier = pipeline(\n    \"text-classification\",\n    model=\"../go_emotions_model\",\n    tokenizer=\"../go_emotions_model\",\n    top_k=None,\n    truncation=True,\n    max_length=128,\n)\n\nprint(\"Classifier ready.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_lyrics(lyrics: str, max_words: int = 80) -> list[str]:\n",
    "    \"\"\"Split lyrics into chunks that fit the model's 128-token window.\"\"\"\n",
    "    lines = lyrics.strip().split(\"\\n\")\n",
    "    chunks, current, current_len = [], [], 0\n",
    "\n",
    "    for line in lines:\n",
    "        n = len(line.split())\n",
    "        if current_len + n > max_words:\n",
    "            if current:\n",
    "                chunks.append(\" \".join(current))\n",
    "            current, current_len = [line], n\n",
    "        else:\n",
    "            current.append(line)\n",
    "            current_len += n\n",
    "\n",
    "    if current:\n",
    "        chunks.append(\" \".join(current))\n",
    "\n",
    "    return chunks or [lyrics]\n",
    "\n",
    "\n",
    "def classify_lyrics(lyrics: str, min_score: float = 0.10) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Classify emotions in song lyrics using the go_emotions fine-tuned model.\n",
    "    Averages scores across lyric chunks and returns emotions above min_score.\n",
    "\n",
    "    Returns: [{\"label\": str, \"score\": float}, ...] sorted by score desc\n",
    "    \"\"\"\n",
    "    chunks = chunk_lyrics(lyrics)\n",
    "    all_results = emotion_classifier(chunks)  # list of lists\n",
    "\n",
    "    label_scores: dict[str, list[float]] = {}\n",
    "    for chunk_result in all_results:\n",
    "        for item in chunk_result:\n",
    "            label_scores.setdefault(item[\"label\"], []).append(item[\"score\"])\n",
    "\n",
    "    averaged = [\n",
    "        {\"label\": label, \"score\": round(float(np.mean(scores)), 4)}\n",
    "        for label, scores in label_scores.items()\n",
    "    ]\n",
    "\n",
    "    filtered = [e for e in averaged if e[\"score\"] >= min_score]\n",
    "    return sorted(filtered, key=lambda x: x[\"score\"], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test it ---\n",
    "sample_lyrics = \"\"\"\n",
    "I been running through the jungle, I been crying with the wolves\n",
    "To get to you, to get to you\n",
    "I been down the darkest alleys, saw the dark side of the moon\n",
    "To get to you, to get to you\n",
    "\"\"\"\n",
    "\n",
    "emotions = classify_lyrics(sample_lyrics)\n",
    "print(\"Emotions:\")\n",
    "for e in emotions:\n",
    "    print(f\"  {e['label']}: {e['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Song similarity matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def embed_lyrics(lyrics: str) -> list[float]:\n",
    "    \"\"\"384-dim normalized embedding for cosine similarity matching.\"\"\"\n",
    "    return embedder.encode(lyrics, normalize_embeddings=True).tolist()\n",
    "\n",
    "\n",
    "def cosine_similarity(a: list[float], b: list[float]) -> float:\n",
    "    a, b = np.array(a), np.array(b)\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "\n",
    "\n",
    "def find_similar_songs(query_lyrics: str, song_library: list[dict], top_k: int = 5) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Find songs with similar lyric content.\n",
    "    song_library: list of Firestore docs — each needs an 'embedding' field.\n",
    "    \"\"\"\n",
    "    query_embedding = embed_lyrics(query_lyrics)\n",
    "    scored = [\n",
    "        {\n",
    "            \"artist\": s[\"artist\"],\n",
    "            \"title\": s[\"title\"],\n",
    "            \"emotions\": s.get(\"emotions\", []),\n",
    "            \"similarity\": cosine_similarity(query_embedding, s[\"embedding\"])\n",
    "        }\n",
    "        for s in song_library if \"embedding\" in s\n",
    "    ]\n",
    "    return sorted(scored, key=lambda x: x[\"similarity\"], reverse=True)[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Full pipeline — returns the object your teammate writes to Firestore ---\n",
    "\n",
    "def analyze_song(artist: str, title: str, lyrics: str) -> dict:\n",
    "    return {\n",
    "        \"artist\": artist,\n",
    "        \"title\": title,\n",
    "        \"lyrics\": lyrics,\n",
    "        \"emotions\": classify_lyrics(lyrics),\n",
    "        \"embedding\": embed_lyrics(lyrics),   # ask teammate to store this for similarity search\n",
    "    }\n",
    "\n",
    "\n",
    "result = analyze_song(\n",
    "    artist=\"Imagine Dragons\",\n",
    "    title=\"Believer\",\n",
    "    lyrics=sample_lyrics\n",
    ")\n",
    "\n",
    "print(f\"{result['artist']} — {result['title']}\")\n",
    "print(\"Top emotions:\", result[\"emotions\"][:5])\n",
    "print(\"Embedding dims:\", len(result[\"embedding\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}